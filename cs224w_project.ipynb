{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s2YZdqm-CAfx8gFCoBmwSwhXd9SeW63h",
      "authorship_tag": "ABX9TyOBVnbzGLJEIniFr7/Wx9cR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jupiterepoch/SGC/blob/master/cs224w_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project for CS224W: Machine Learning with Graphs\n",
        "Chenshu Zhu\\*, Xinglong Sun\\*, Ziang Liu\\*\n",
        "{chenshu, xs15, ziangliu}@stanford.edu\n",
        "\n",
        "\\* equal contributions"
      ],
      "metadata": {
        "id": "cgLf3eEBUorL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Getting started with SGC\n",
        "\n",
        "TODO: math formulation of SGC and a picture of the model architecture\n",
        "\n",
        "In this part we replicate the results with the official code released by the SGC authors. Then we go on to explore the limitations of the SGC formulation, and experiment with different techniques to mitigate these limitations.\n",
        "\n"
      ],
      "metadata": {
        "id": "XktN6SGxUjQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation\n",
        "This combines the environment dependencies for both part1 and part2 to ensure a hassle-free code running process through this notebook."
      ],
      "metadata": {
        "id": "lh4YU4aVSb96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrzDSK-FQ5NP",
        "outputId": "2848e598-4d64-4394-8521-e4df8e88735b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=9d078449ad0479a5a4e119c750d73327787b4edea5135613a5afcb7def92170b\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.2.2)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.4.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.13.1+cu116)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.26.15)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (63.4.3)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=e506c01a42a531ccaef055914525bd262636507e59e897c18b1db7c33a2122e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "! pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
        "! pip install torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
        "! pip install torch-geometric\n",
        "! pip install ogb\n",
        "! pip install networkx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the following 3 code blocks even if you just want to play with Part 2"
      ],
      "metadata": {
        "id": "cDKNgaA-iVkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seed the notebook for more stability\n",
        "import numpy as np\n",
        "import random\n",
        "import torch_geometric\n",
        "import torch\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch_geometric.seed_everything(seed)\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "hLhjbqjebK9j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class objectview(object):\n",
        "    def __init__(self, d):\n",
        "        self.__dict__ = d"
      ],
      "metadata": {
        "id": "d6-_7FoAiRt0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.cuda.is_available(), print('please attach to a GPU runtime')\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "TKGHU8pbUj4I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A GPU runtime is required for the notebook"
      ],
      "metadata": {
        "id": "qTdPw9i4UljC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TODO:\n",
        "\n",
        "You also need to mount the google drive and have a data folder to run the first part"
      ],
      "metadata": {
        "id": "71fx9x6vwdXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages for Part 1"
      ],
      "metadata": {
        "id": "GYCyZwe_VytS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import sys\n",
        "from time import perf_counter\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from torch_geometric.nn import SGConv\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing"
      ],
      "metadata": {
        "id": "O-6zt7YDYCIg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's start by defining the SGC model \n",
        "\n",
        "The SGC model can be viewed as a feature encoder. Given a graph G, SGC conducts a k-round neighborhood aggregation of initial features, thus encoding the graph structure into the aggregated node embeddings. After this embedding is extracted, any decoder network can be trained on it for downstream tasks. The essential part of the SGC thus lies in its encoder, or \"precompute\" stage as named by the authors."
      ],
      "metadata": {
        "id": "gYX0JkxaYPGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgc_precompute(features, adj, degree):\n",
        "    \"\"\"\n",
        "    The most basic SGC model\n",
        "    \"\"\"\n",
        "    t = perf_counter()\n",
        "    for i in range(degree):\n",
        "        features = torch.spmm(adj, features)\n",
        "    precompute_time = perf_counter()-t\n",
        "    return features, precompute_time\n",
        "\n",
        "def sgc_precompute_concat(features, adj, degree):\n",
        "    \"\"\"\n",
        "    Augmented SGC model with residual connections\n",
        "    \"\"\"\n",
        "    t = perf_counter()\n",
        "    total = [deepcopy(features)]\n",
        "    for i in range(degree):\n",
        "        features = torch.spmm(adj, features)\n",
        "        total.append(deepcopy(features))\n",
        "    precompute_time = perf_counter()-t\n",
        "    total = torch.cat(total, dim=-1)\n",
        "    return total, precompute_time\n",
        "\n",
        "def aug_normalized_adjacency(adj):\n",
        "    \"\"\"\n",
        "    Normalize the adjacency matrix as defined in SGC paper\n",
        "    \"\"\"\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    row_sum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()"
      ],
      "metadata": {
        "id": "DJUjU8uvY-qG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we move on to building the decoders for the SGC model"
      ],
      "metadata": {
        "id": "DNDg1HfQaRua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGC(nn.Module):\n",
        "    \"\"\"\n",
        "    A Simple PyTorch Implementation of Logistic Regression.\n",
        "    Assuming the features have been preprocessed with k-step graph propagation.\n",
        "    \"\"\"\n",
        "    def __init__(self, nfeat, nclass):\n",
        "        super(SGC, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(nfeat, nclass)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.W(x)\n",
        "\n",
        "class SGC_Big(nn.Module):\n",
        "    \"\"\"\n",
        "    A Simple PyTorch Implementation of Logistic Regression.\n",
        "    Assuming the features have been preprocessed with k-step graph propagation.\n",
        "    \"\"\"\n",
        "    def __init__(self, nfeat, nclass, nhid):\n",
        "        super(SGC_Big, self).__init__()\n",
        "        self.W1 = nn.Linear(nfeat, nhid)\n",
        "        self.W2 = nn.Linear(nhid, nclass)\n",
        "        self.dropout = 0.9\n",
        "\n",
        "    def forward(self, x, use_relu=True):\n",
        "        x = self.W1(x)\n",
        "        if use_relu:\n",
        "            x = F.relu(x)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.W2(x)\n",
        "        return x\n",
        "\n",
        "class SGC_Big_CS(nn.Module):\n",
        "    \"\"\"\n",
        "    A Simple PyTorch Implementation of Logistic Regression.\n",
        "    Assuming the features have been preprocessed with k-step graph propagation.\n",
        "    \"\"\"\n",
        "    def __init__(self, nfeat, nclass, nhid):\n",
        "        super(SGC_Big, self).__init__()\n",
        "        self.W1 = nn.Linear(nfeat, nhid)\n",
        "        self.W2 = nn.Linear(nhid, nclass)\n",
        "        self.dropout = 0.9\n",
        "\n",
        "    def forward(self, x, use_relu=True):\n",
        "        x = self.W1(x)\n",
        "        if use_relu:\n",
        "            x = F.relu(x)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.W2(x)\n",
        "        return x\n",
        "\n",
        "class LPAconv(MessagePassing):\n",
        "    \"\"\"\n",
        "    LPA convolution operation.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input node features of shape [num_nodes, num_node_features].\n",
        "        edge_index (LongTensor or SparseTensor): Graph edge indices of shape [2, num_edges].\n",
        "        mask (bool Tensor, optional): Mask for selecting a subset of nodes to operate on.\n",
        "        edge_weight (float Tensor, optional): Edge weights of shape [num_edges].\n",
        "        post_step (function, optional): Post-processing function applied to output.\n",
        "        \n",
        "    Returns:\n",
        "        Tensor: Output node features of shape [num_nodes, num_node_features].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers: int):\n",
        "        super(LPAconv, self).__init__(aggr='add')\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(\n",
        "            self, y, edge_index, mask = None,\n",
        "            edge_weight = None,\n",
        "            post_step = lambda y: y.clamp_(0., 1.)\n",
        "    ):\n",
        "\n",
        "        if y.dtype == torch.int64:\n",
        "            y = F.one_hot(y.view(-1)).to(torch.float)\n",
        "\n",
        "        out = y\n",
        "        if mask is not None:\n",
        "            out = torch.zeros_like(y)\n",
        "            out[mask] = y[mask]\n",
        "\n",
        "        if isinstance(edge_index, SparseTensor) and not edge_index.has_value():\n",
        "            edge_index = gcn_norm(edge_index, add_self_loops=False)\n",
        "        elif isinstance(edge_index, Tensor) and edge_weight is None:\n",
        "            edge_index, edge_weight = gcn_norm(edge_index, num_nodes=y.size(0),\n",
        "                                              add_self_loops=False)\n",
        "\n",
        "        for _ in range(self.num_layers):\n",
        "            # propagate_type: (y: Tensor, edge_weight: OptTensor)\n",
        "            out = self.propagate(edge_index, x=out, edge_weight=edge_weight,\n",
        "                                size=None)\n",
        "        return out\n",
        "\n",
        "class SGC_LPA(nn.Module):\n",
        "    \"\"\"\n",
        "    A class representing the SGC-LPA model.\n",
        "\n",
        "    Args:\n",
        "        in_feature (int): Number of input features.\n",
        "        hidden (int): Number of hidden features.\n",
        "        out_feature (int): Number of output features.\n",
        "        dropout (float): Dropout rate to use.\n",
        "        num_edges (int): Number of edges in the graph.\n",
        "        lpaiters (int): Number of LPA iterations to perform.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_feature, hidden, out_feature, dropout, num_edges, lpaiters):\n",
        "        super(SGC_LPA, self).__init__()\n",
        "        self.edge_weight = nn.Parameter(torch.ones(num_edges))\n",
        "        self.conv1 = SGConv(in_feature, out_feature, K=2, cached=True)\n",
        "        self.lpa = LPAconv(lpaiters)\n",
        "        self.dropout_rate = dropout\n",
        "\n",
        "    def forward(self, x, adj, y, mask):\n",
        "        x, edge_index, y = data.x, data.edge_index, data.y\n",
        "        x = self.conv1(x, adj, edge_weight=self.edge_weight)\n",
        "        y_hat = self.lpa(y, edge_index, mask, self.edge_weight)\n",
        "\n",
        "        return x, y_hat\n"
      ],
      "metadata": {
        "id": "rq5m0tsXYOKh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_opt, nfeat, nclass, nhid=0, dropout=0, num_edges=None):\n",
        "    \"\"\"\n",
        "    A helper function that chooses between different augmentations of SGC\n",
        "    \"\"\"\n",
        "\n",
        "    if model_opt == \"SGC\":\n",
        "        model = SGC(nfeat=nfeat, nclass=nclass)\n",
        "    elif model_opt == \"SGC-Concat\":\n",
        "        model = SGC_Big(nfeat=nfeat, nclass=nclass, nhid=nhid)\n",
        "    elif model_opt == \"SGC-LPA\":\n",
        "        model = SGC_LPA(in_feature=nfeat, out_feature=nclass, hidden=nhid, dropout=0.9, num_edges=num_edges, lpaiters=2)\n",
        "    else:\n",
        "        raise NotImplementedError('model:{} is not implemented!'.format(model_opt))\n",
        "\n",
        "    return model.cuda()"
      ],
      "metadata": {
        "id": "nN5BQw9uqLEB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the data loaders\n",
        "\n",
        "Many functions are defined for loading and pre-processing data"
      ],
      "metadata": {
        "id": "tjuoeMexi1zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_normalization(type):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    switcher = {\n",
        "        'AugNormAdj': aug_normalized_adjacency,  # A' = (D + I)^-1/2 * ( A + I ) * (D + I)^-1/2\n",
        "    }\n",
        "    func = switcher.get(type, lambda: \"Invalid normalization technique.\")\n",
        "    return func\n",
        "\n",
        "def row_normalize(mx):\n",
        "    \"\"\"\n",
        "    Row-normalize sparse matrix\n",
        "    \"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"\n",
        "    Parse index file.\n",
        "    \"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "def generate_DAD(adj):\n",
        "    \"\"\"\n",
        "    Computes the DAD matrix for a sparse adjacency matrix. D is the degree matrix.\n",
        "    \"\"\"\n",
        "    adj = adj\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    row_sum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    DAD = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
        "    DA = d_mat_inv_sqrt.dot(adj)\n",
        "    return DAD.tocoo(), DA.tocoo()\n",
        "\n",
        "def preprocess_citation(adj, features, normalization=\"FirstOrderGCN\"):\n",
        "    \"\"\"\n",
        "    Preprocess dataset\n",
        "    \"\"\"\n",
        "    adj_normalizer = fetch_normalization(normalization)\n",
        "    adj = adj_normalizer(adj)\n",
        "    features = row_normalize(features)\n",
        "    return adj, features\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"\n",
        "    Convert a scipy sparse matrix to a torch sparse tensor.\n",
        "    \"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def load_citation(path, dataset_str=\"cora\", normalization=\"AugNormAdj\"):\n",
        "    \"\"\"\n",
        "    Load Citation Networks Datasets.\n",
        "    \"\"\"\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"{}/ind.{}.{}\".format(path, dataset_str.lower(), names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\"{}/ind.{}.test.index\".format(path, dataset_str))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    if dataset_str == 'citeseer':\n",
        "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "        # Find isolated nodes, add them as zero-vecs into the right position\n",
        "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "        ty = ty_extended\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "    DAD, DA = generate_DAD(adj)\n",
        "    # Calculate number of edges\n",
        "    num_edges = np.sum(adj.data) // 2\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "    idx_test = test_idx_range.tolist()\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y)+500)\n",
        "\n",
        "    adj, features = preprocess_citation(adj, features, normalization)\n",
        "\n",
        "    # porting to pytorch\n",
        "    features = torch.FloatTensor(np.array(features.todense())).float()\n",
        "    labels = torch.LongTensor(labels)\n",
        "    labels = torch.max(labels, dim=1)[1]\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
        "    DAD = sparse_mx_to_torch_sparse_tensor(DAD).float()\n",
        "    DA = sparse_mx_to_torch_sparse_tensor(DA).float()\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    # moving things to GPU for later training\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    DAD = DAD.cuda()\n",
        "    DA = DA.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test, num_edges, DAD, DA"
      ],
      "metadata": {
        "id": "MW7351oxi1AU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now define the driver function for training and testing the SGC model"
      ],
      "metadata": {
        "id": "TfmDYkuMX7mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a trainer function that trains a GNN model\n",
        "def train_regression(model,\n",
        "                     train_features, train_labels,\n",
        "                     val_features, val_labels,\n",
        "                     epochs, weight_decay,\n",
        "                     lr, dropout, need_adj_fwd=False, adj=None, val_adj=None):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "    t = perf_counter()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        if not need_adj_fwd:\n",
        "            output = model(train_features)\n",
        "        else:\n",
        "            output = model(train_features, adj=adj)\n",
        "        loss_train = F.cross_entropy(output, train_labels)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "    train_time = perf_counter()-t\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        if not need_adj_fwd:\n",
        "            output = model(val_features)\n",
        "        else:\n",
        "            output = model(val_features, adj=val_adj)\n",
        "        acc_val = accuracy(output, val_labels)\n",
        "\n",
        "        # print(f'Epoch {epoch+1},  val accuracy: {acc_val}  loss: {loss_train.item()}')\n",
        "\n",
        "    return model, acc_val, train_time\n"
      ],
      "metadata": {
        "id": "_XUckeb5WQoD"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a trainer function that applies correct and smooth after trains a GNN model\n",
        "def train_regression(model,\n",
        "                     train_features, train_labels,\n",
        "                     val_features, val_labels,\n",
        "                     epochs=args.epochs, weight_decay=args.weight_decay,\n",
        "                     lr=args.lr, dropout=args.dropout, need_adj_fwd=False, adj=None, val_adj=None, all_features=None, post_cs=None, train_idx=None, DAD=None, DA=None, val_idx=None, test_idx=None, test_labels=None):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "    t = perf_counter()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        if not need_adj_fwd:\n",
        "            output = model(train_features)\n",
        "        else:\n",
        "            output = model(train_features, adj=adj)\n",
        "        loss_train = F.cross_entropy(output, train_labels)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "    train_time = perf_counter()-t\n",
        "    model.eval()\n",
        "    if all_features is not None and post_cs is not None:\n",
        "        with torch.no_grad():\n",
        "            output = model(all_features)\n",
        "        y_soft = output.softmax(dim=-1)\n",
        "        DAD = DAD.to_dense()\n",
        "        indices = torch.nonzero(DAD).t()\n",
        "        DAD = SparseTensor(row=indices[0], col=indices[1], value=DAD[indices[0], indices[1]], sparse_sizes=DAD.size())\n",
        "\n",
        "        DA = DA.to_dense()\n",
        "        indices = torch.nonzero(DA).t()\n",
        "        DA = SparseTensor(row=indices[0], col=indices[1], value=DA[indices[0], indices[1]], sparse_sizes=DA.size())\n",
        "        y_soft = post_cs.correct(y_soft=y_soft, y_true=train_labels.unsqueeze(-1), mask=train_idx, edge_index=DAD)\n",
        "        y_soft = post_cs.smooth(y_soft=y_soft, y_true=train_labels.unsqueeze(-1), mask=train_idx, edge_index=DAD)\n",
        "        acc_val = accuracy(y_soft[val_idx], val_labels)\n",
        "        acc_test = accuracy(y_soft[test_idx], test_labels)\n",
        "\n",
        "    return model, acc_val, train_time, acc_test"
      ],
      "metadata": {
        "id": "t-YN55-Q4pT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now define the metrics and tester\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "def test_regression(model, test_features, test_labels, need_adj_fwd=False, adj=None):\n",
        "    model.eval()\n",
        "    if not need_adj_fwd:\n",
        "        return accuracy(model(test_features), test_labels)\n",
        "    else:\n",
        "        return accuracy(model(test_features, adj), test_labels)"
      ],
      "metadata": {
        "id": "xZo7Oz9sh64Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = objectview({\n",
        "    'epochs': 250,\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-6,\n",
        "    'hidden': 128,\n",
        "    'dropout': 0.5,\n",
        "    'dataset': 'cora',\n",
        "    'normalization': 'AugNormAdj',\n",
        "    'degree': 5,\n",
        "})"
      ],
      "metadata": {
        "id": "7haaPgv-klqe"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you need to connect to a drive with data\n",
        "\n",
        "path = 'drive/MyDrive/data'\n",
        "\n",
        "def main():\n",
        "    adj, features, labels, idx_train, idx_val, idx_test, num_edges, DAD, DA = load_citation(path, args.dataset, args.normalization)\n",
        "\n",
        "    features, precompute_time = sgc_precompute_concat(features, adj, args.degree)\n",
        "    \n",
        "    nfeat = features.size(1) # * (args.degree+1)\n",
        "    nclass = labels.max().item()+1\n",
        "\n",
        "    model = get_model('SGC-Concat', nfeat=nfeat, nclass=nclass, nhid=args.hidden, dropout=args.dropout)\n",
        "\n",
        "    model, acc_val, train_time = train_regression(\n",
        "                     model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
        "                     args.epochs, args.weight_decay, args.lr, args.dropout)\n",
        "    \n",
        "    acc_test = test_regression(model, features[idx_test], labels[idx_test])\n",
        "\n",
        "    print(\"Validation Accuracy: {:.4f}  Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
        "    print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(precompute_time, train_time, precompute_time+train_time))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPwugKe_Z7ob",
        "outputId": "3b9b6ef3-667b-4c26-bc11-e2c5e6da7b65"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-9729596bd5ad>:60: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
            "  objects.append(pkl.load(f, encoding='latin1'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7960  Test Accuracy: 0.8310\n",
            "Pre-compute time: 0.0132s, train time: 0.3452s, total: 0.3584s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Applying SGC to real-world problems\n",
        "\n",
        "In this part, we practice what we learned from the thorough experimentation in Part 1, and verify whether our augmentation works at scale (robust enough to generalize to real-world datasets). Specifically, we re-implement SGC under the OGB framework, and test our methods on OGB node classification datasets. Empirical results demonstrate the effectiveness of our model augmentation."
      ],
      "metadata": {
        "id": "aOmV-IpWSL1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required packages for Part 2\n",
        "\n",
        "Many packages are repeating from Part 1, but we list them here in case you just want to play with Part 2\n",
        "\n"
      ],
      "metadata": {
        "id": "uH16ubG1THtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "import torch_geometric\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import CorrectAndSmooth\n",
        "from torch_geometric.nn import JumpingKnowledge\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.dense.linear import Linear\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from ogb.nodeproppred import Evaluator as NodeEvaluator\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "63nJWngITG2M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Construction"
      ],
      "metadata": {
        "id": "mr-AFkRjSltg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to be able to add functionalities to SGC easily, also we would like to have outputs with the right shapes for different datasets. Thus we define a SGC wrapper class. The wrapper class ```SGC``` calls a ```SGConv``` layer which does the SGC propagations, and uses a linear layer to project to the dimension we want. This way, tweaking the ```SGConv``` layer is very easy. The base SGC method is achieved through simple sum aggregation and adjacency matrix normalization provided by ```torch_geometrics```."
      ],
      "metadata": {
        "id": "T0_QJtUXYdD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGC(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The SGC wrapper class: computes the SGC encoding of network features, \n",
        "    then projects to the number of output classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5, K=3, aug=None):\n",
        "        super(SGC, self).__init__()\n",
        "        if not aug:\n",
        "            self.conv = SGConv(in_channels, hidden_channels, K=K)\n",
        "        elif aug == 'pyramid':\n",
        "            self.conv = SGConvRes(in_channels, hidden_channels, K=K)\n",
        "        elif aug == 'deepres':\n",
        "            self.conv = DRSGConv(in_channels, hidden_channels, K=K)\n",
        "        elif aug == 'jk':\n",
        "            self.conv = SGConvJK(in_channels, hidden_channels, K=K)\n",
        "        self.lin = Linear(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv.reset_parameters()\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        x = self.conv(x, adj_t, edge_weight=adj_t)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "EVyBWU-LSlEE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following blocks define the base SGC convolutional layer and several augmentation schemes."
      ],
      "metadata": {
        "id": "8RgRnLCkeupe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGConv(MessagePassing):\n",
        "    \"\"\"\n",
        "    The simple graph convolutional operator from the `\"Simplifying Graph\n",
        "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
        "                 add_self_loops: bool = True, bias: bool = True, **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj,\n",
        "                edge_weight: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if isinstance(edge_index, Tensor):\n",
        "            edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
        "                edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                self.add_self_loops, dtype=x.dtype)\n",
        "        elif isinstance(edge_index, SparseTensor):\n",
        "            edge_index = gcn_norm(  # yapf: disable\n",
        "                edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                self.add_self_loops, dtype=x.dtype)\n",
        "        for k in range(self.K):\n",
        "            # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
        "            x = self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None)\n",
        "        return self.lin(x)\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, K={self.K})')"
      ],
      "metadata": {
        "id": "g6yQuoepS4qW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGConvRes(MessagePassing):\n",
        "    \"\"\"\n",
        "    SGC with residual connections from all propagation depths\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
        "                 add_self_loops: bool = True, bias: bool = True, **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.lin = Linear((K+1) * in_channels, out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj,\n",
        "                edge_weight: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if isinstance(edge_index, Tensor):\n",
        "            edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
        "                edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                self.add_self_loops, dtype=x.dtype)\n",
        "        elif isinstance(edge_index, SparseTensor):\n",
        "            edge_index = gcn_norm(  # yapf: disable\n",
        "                edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                self.add_self_loops, dtype=x.dtype)\n",
        "        total = [deepcopy(x.detach())]\n",
        "        for k in range(self.K):\n",
        "            # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
        "            x = self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None)\n",
        "            total.append(deepcopy(x.detach()))\n",
        "        total = torch.cat(total, dim=-1)\n",
        "        return self.lin(total)\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, K={self.K})')"
      ],
      "metadata": {
        "id": "OYDoXwjvgMrp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DRSGConv(MessagePassing):\n",
        "    \"\"\"\n",
        "    SGC with RNN-gated residual connections from the first propagation layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
        "                 add_self_loops: bool = True, bias: bool = True, **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.add_self_loops = add_self_loops\n",
        "\n",
        "        self.rnn = nn.GRU(1, 1, num_layers=1, batch_first=True)\n",
        "        self.lin = Linear(in_channels, 1, bias=bias)\n",
        "        self.lin_out = Linear(in_channels, out_channels, bias=bias)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin.reset_parameters()\n",
        "        self.lin_out.reset_parameters()\n",
        "        self.rnn.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj,\n",
        "                edge_weight: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if isinstance(edge_index, Tensor):\n",
        "            edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
        "                edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                self.add_self_loops, dtype=x.dtype)\n",
        "        elif isinstance(edge_index, SparseTensor):\n",
        "            edge_index = gcn_norm(  # yapf: disable\n",
        "                edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                self.add_self_loops, dtype=x.dtype)\n",
        "        x = self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None)\n",
        "        x0 = deepcopy(x.detach())\n",
        "        h0 = torch.nn.Parameter(torch.rand(1, len(x0), 1) * (1.0 / self.out_channels) ** 0.5).to(x0.device)\n",
        "        for k in range(self.K-1):\n",
        "            # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
        "            x = self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None)\n",
        "            z = self.lin(F.normalize(x) * F.normalize(x0))[:, None]\n",
        "            alpha, h0 = self.rnn(z, h0)\n",
        "            alpha = torch.abs(alpha).squeeze(1)\n",
        "            x = (1 - alpha) * x + alpha * x0\n",
        "        return self.lin_out(x)"
      ],
      "metadata": {
        "id": "XVArKlChcHWq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGConvJK(MessagePassing):\n",
        "    \"\"\"\n",
        "    SGC with Jumping Knowledge\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
        "                 add_self_loops: bool = True, bias: bool = True, **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.add_self_loops = add_self_loops\n",
        "\n",
        "        self.jk = JumpingKnowledge(mode='max', channels=in_channels, num_layers=K+1)\n",
        "        self.lin = Linear(in_channels, out_channels)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin.reset_parameters()\n",
        "        self.jk.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj,\n",
        "                edge_weight: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if isinstance(edge_index, Tensor):\n",
        "            edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
        "                edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                self.add_self_loops, dtype=x.dtype)\n",
        "        elif isinstance(edge_index, SparseTensor):\n",
        "            edge_index = gcn_norm(  # yapf: disable\n",
        "                edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                self.add_self_loops, dtype=x.dtype)\n",
        "        total = [deepcopy(x.detach())]\n",
        "        for k in range(self.K):\n",
        "            # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
        "            x = self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None)\n",
        "            total.append(deepcopy(x.detach()))\n",
        "        x = self.jk(total)\n",
        "        return self.lin(x)\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, K={self.K})')"
      ],
      "metadata": {
        "id": "_uri_dKgW6eM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing driver codes"
      ],
      "metadata": {
        "id": "wqXvWnV8VTVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_node(model, data, split_node, optimizer, batch_size):\n",
        "    '''\n",
        "    Trains a given GNN model on the provided data for node classification\n",
        "    '''\n",
        "    model.train()\n",
        "\n",
        "    total_loss = total_examples = 0\n",
        "    for perm in DataLoader(split_node['train'], batch_size, shuffle=True):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # compute SGC propagations\n",
        "        h = model(data.x, data.adj_t)\n",
        "\n",
        "        # logits are log probabilities for each class\n",
        "        logits = h[perm]\n",
        "        labels = data.y[perm].squeeze(-1)\n",
        "\n",
        "        # nll_loss for multi-class classification\n",
        "        loss = F.nll_loss(logits, labels, reduction='sum')\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(split_node['train'])\n"
      ],
      "metadata": {
        "id": "_yXfM1G6UbSH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(y_pred, y_true, split_node, evaluator):\n",
        "    '''\n",
        "    Calculates the node prediciton accuracy given a OGB evaluator\n",
        "    '''\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_node['train']],\n",
        "        'y_pred': y_pred[split_node['train']],\n",
        "    })[f'acc']\n",
        "\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_node['valid']],\n",
        "        'y_pred': y_pred[split_node['valid']],\n",
        "    })[f'acc']\n",
        "\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_node['test']],\n",
        "        'y_pred': y_pred[split_node['test']],\n",
        "    })[f'acc']\n",
        "\n",
        "    result_dict = {\n",
        "        'train': train_acc,\n",
        "        'valid': valid_acc,\n",
        "        'test': test_acc\n",
        "    }\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_node(model, data, split_node, evaluator, correct_smooth=False):\n",
        "    '''\n",
        "    Evaluates the model using the ogb evaluator, if correct and smooth is set\n",
        "    to true, then performs correct&smooth on the predictor results\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    h = model(data.x, data.adj_t)\n",
        "    y_soft = h.softmax(-1)\n",
        "\n",
        "    y_pred = y_soft.argmax(-1, keepdims=True)\n",
        "    result = get_metrics(y_pred, data.y, split_node, evaluator)\n",
        "    result_smoothed = None\n",
        "\n",
        "    if correct_smooth:\n",
        "\n",
        "        deg = data.adj_t.sum(dim=1).to(torch.float)\n",
        "        deg_inv_sqrt = deg.pow_(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        DAD = deg_inv_sqrt.view(-1, 1) * data.adj_t * deg_inv_sqrt.view(1, -1)\n",
        "        DA = deg_inv_sqrt.view(-1, 1) * deg_inv_sqrt.view(-1, 1) * data.adj_t\n",
        "\n",
        "        post = CorrectAndSmooth(num_correction_layers=50, correction_alpha=0.9,\n",
        "                                num_smoothing_layers=50, smoothing_alpha=0.8,\n",
        "                                autoscale=True, scale=20.)\n",
        "        \n",
        "        y_true = data.y[split_node['train']]\n",
        "        y_soft = post.correct(y_soft, y_true, split_node['train'], DAD)\n",
        "        y_soft = post.smooth(y_soft, y_true, split_node['train'], DAD) # DAD performs better than DA\n",
        "        y_pred = y_soft.argmax(-1, keepdims=True)\n",
        "\n",
        "        result_smoothed = get_metrics(y_pred, data.y, split_node, evaluator)\n",
        "\n",
        "    return (result, result_smoothed)"
      ],
      "metadata": {
        "id": "ZJzXF-yjiRG4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment:\n",
        "First train a simple SGC model on the ogbn-arxiv dataset. Then we compare the effectiveness of different augmentations. Each time the results with and without using correct & smooth are displayed alongside for comparison."
      ],
      "metadata": {
        "id": "A_Pm-duvj_x5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Dataloaders from OGB"
      ],
      "metadata": {
        "id": "ihhAs9yGUHli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a real-world dataset for analysis\n",
        "dataset = PygNodePropPredDataset('ogbn-arxiv', transform=T.ToSparseTensor())\n",
        "# get the graph from dataset\n",
        "data = dataset[0]\n",
        "data.x = data.x.to(torch.float)\n",
        "data.adj_t = data.adj_t.to_symmetric()\n",
        "data = data.to(device)\n",
        "split_node = dataset.get_idx_split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhhETFSXUwn8",
        "outputId": "35f01024-ffab-4475-f057-080a30693a74"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:02<00:00, 33.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2150.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 73.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The final driver code that puts everything together.\n",
        "def run(args):\n",
        "    model = SGC(data.num_features, args.hidden_channels, dataset.num_classes, args.dropout, args.K, args.aug).to(device)\n",
        "    sum_params = 0.\n",
        "    for p in model.parameters():\n",
        "        sum_params += p.numel()\n",
        "    print(f'Params: {sum_params}')\n",
        "\n",
        "    model.reset_parameters()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "    evaluator = NodeEvaluator(name='ogbn-arxiv')\n",
        "\n",
        "    best_model = None\n",
        "    best_val = 0.\n",
        "\n",
        "    for epoch in range(1, 1 + args.epochs):\n",
        "        loss = train_node(model, data, split_node, optimizer, args.batch_size)\n",
        "        result, result_smooth = test_node(model, data, split_node, evaluator, correct_smooth=True)\n",
        "\n",
        "        if result['valid'] > best_val:\n",
        "            best_val = result['valid']\n",
        "            best_model = deepcopy(model)\n",
        "\n",
        "        print(f'Epoch: {epoch}, ', end=' ')\n",
        "        for key, val in result.items():\n",
        "            print(f'{key}: {val:.7f}, ', end=' ')\n",
        "        if result_smooth:\n",
        "            print('Correct and smooth... ', end = ' ')\n",
        "            for key, val in result_smooth.items():\n",
        "                print(f'{key}: {val:.7f}, ', end=' ')\n",
        "        print(f'loss: {loss:.7f}, ')\n",
        "\n",
        "    result, result_smooth = test_node(best_model, data, split_node, evaluator, correct_smooth=True)\n",
        "    best_acc, best_acc_smooth = result['test'], result_smooth['test']\n",
        "    print(f\"Best test accuracy is {best_acc:.7f} and {best_acc_smooth:.7f} after correct & smooth\")\n",
        "\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "ZYEt5dPXZAUq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the hyper-parameters for a base run\n",
        "args = objectview({\n",
        "    'hidden_channels': 64,\n",
        "    'dropout': 0.5,\n",
        "    'K': 5,\n",
        "    'lr': 5e-3,\n",
        "    'epochs': 1000,\n",
        "    'batch_size': 1024 * 64,\n",
        "    'aug': None\n",
        "})\n",
        "\n",
        "run(args)"
      ],
      "metadata": {
        "id": "lUjrB181ZLc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "outputId": "b29e3bec-f4c5-45c4-ff3b-98d09eab147c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: 10792.0\n",
            "Epoch: 1,  train: 0.1790611,  valid: 0.0762777,  test: 0.0586178,  Correct and smooth...  train: 0.9536183,  valid: 0.6845196,  test: 0.6592597,  loss: 3.6772354, \n",
            "Epoch: 2,  train: 0.1790611,  valid: 0.0762777,  test: 0.0586178,  Correct and smooth...  train: 0.9535743,  valid: 0.6859626,  test: 0.6613789,  loss: 3.5028514, \n",
            "Epoch: 3,  train: 0.1790611,  valid: 0.0762777,  test: 0.0586178,  Correct and smooth...  train: 0.9536403,  valid: 0.6881439,  test: 0.6628809,  loss: 3.3409822, \n",
            "Epoch: 4,  train: 0.1790611,  valid: 0.0762777,  test: 0.0586178,  Correct and smooth...  train: 0.9537722,  valid: 0.6895869,  test: 0.6648972,  loss: 3.2190938, \n",
            "Epoch: 5,  train: 0.1790611,  valid: 0.0762777,  test: 0.0586178,  Correct and smooth...  train: 0.9536403,  valid: 0.6961643,  test: 0.6736621,  loss: 3.1639033, \n",
            "Epoch: 6,  train: 0.2601247,  valid: 0.2681298,  test: 0.2500257,  Correct and smooth...  train: 0.9534753,  valid: 0.7041176,  test: 0.6867066,  loss: 3.1402600, \n",
            "Epoch: 7,  train: 0.2678550,  valid: 0.2844726,  test: 0.2603337,  Correct and smooth...  train: 0.9532994,  valid: 0.7033122,  test: 0.6867066,  loss: 3.1164730, \n",
            "Epoch: 8,  train: 0.2109280,  valid: 0.1339642,  test: 0.1120095,  Correct and smooth...  train: 0.9533104,  valid: 0.6997886,  test: 0.6799786,  loss: 3.0796636, \n",
            "Epoch: 9,  train: 0.1794570,  valid: 0.0763784,  test: 0.0586795,  Correct and smooth...  train: 0.9533764,  valid: 0.6950904,  test: 0.6724276,  loss: 3.0374708, \n",
            "Epoch: 10,  train: 0.1796769,  valid: 0.0764120,  test: 0.0587412,  Correct and smooth...  train: 0.9534753,  valid: 0.6932447,  test: 0.6694854,  loss: 3.0040008, \n",
            "Epoch: 11,  train: 0.2103012,  valid: 0.1320850,  test: 0.1109602,  Correct and smooth...  train: 0.9533654,  valid: 0.6939495,  test: 0.6701232,  loss: 2.9672592, \n",
            "Epoch: 12,  train: 0.2642592,  valid: 0.2858150,  test: 0.2739543,  Correct and smooth...  train: 0.9532224,  valid: 0.6972046,  test: 0.6742382,  loss: 2.9259596, \n",
            "Epoch: 13,  train: 0.2782793,  valid: 0.3077620,  test: 0.2892208,  Correct and smooth...  train: 0.9532994,  valid: 0.7007282,  test: 0.6788676,  loss: 2.8794234, \n",
            "Epoch: 14,  train: 0.2827877,  valid: 0.3081647,  test: 0.2831307,  Correct and smooth...  train: 0.9534094,  valid: 0.7039834,  test: 0.6839701,  loss: 2.8343499, \n",
            "Epoch: 15,  train: 0.2841183,  valid: 0.3071580,  test: 0.2784808,  Correct and smooth...  train: 0.9536293,  valid: 0.7064331,  test: 0.6882291,  loss: 2.7822081, \n",
            "Epoch: 16,  train: 0.2919695,  valid: 0.3108494,  test: 0.2803325,  Correct and smooth...  train: 0.9536733,  valid: 0.7074398,  test: 0.6906158,  loss: 2.7332148, \n",
            "Epoch: 17,  train: 0.3122354,  valid: 0.3224940,  test: 0.2944468,  Correct and smooth...  train: 0.9536953,  valid: 0.7080439,  test: 0.6919326,  loss: 2.6829758, \n",
            "Epoch: 18,  train: 0.3392199,  valid: 0.3430652,  test: 0.3153715,  Correct and smooth...  train: 0.9536073,  valid: 0.7085137,  test: 0.6920766,  loss: 2.6318615, \n",
            "Epoch: 19,  train: 0.3610033,  valid: 0.3580993,  test: 0.3318314,  Correct and smooth...  train: 0.9536073,  valid: 0.7090842,  test: 0.6918091,  loss: 2.5781652, \n",
            "Epoch: 20,  train: 0.3751443,  valid: 0.3688043,  test: 0.3438059,  Correct and smooth...  train: 0.9538382,  valid: 0.7092855,  test: 0.6920560,  loss: 2.5253213, \n",
            "Epoch: 21,  train: 0.3873940,  valid: 0.3799456,  test: 0.3538670,  Correct and smooth...  train: 0.9539372,  valid: 0.7096547,  test: 0.6925704,  loss: 2.4729255, \n",
            "Epoch: 22,  train: 0.4030745,  valid: 0.3926306,  test: 0.3653890,  Correct and smooth...  train: 0.9539262,  valid: 0.7104601,  test: 0.6932700,  loss: 2.4230660, \n",
            "Epoch: 23,  train: 0.4221968,  valid: 0.4077989,  test: 0.3795650,  Correct and smooth...  train: 0.9539042,  valid: 0.7105272,  test: 0.6943399,  loss: 2.3724573, \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-439d706589a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m })\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-22d75ad66093>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_smooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-36ceab7c18a0>\u001b[0m in \u001b[0;36mtest_node\u001b[0;34m(model, data, split_node, evaluator, correct_smooth)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_soft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mresult_smoothed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_smoothed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-36ceab7c18a0>\u001b[0m in \u001b[0;36mget_metrics\u001b[0;34m(y_pred, y_true, split_node, evaluator)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     train_acc = evaluator.eval({\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;34m'y_true'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     })[f'acc']\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pyramid SGC\n",
        "args = objectview({\n",
        "    'hidden_channels': 64,\n",
        "    'dropout': 0.5,\n",
        "    'K': 5,\n",
        "    'lr': 5e-3,\n",
        "    'epochs': 1000,\n",
        "    'batch_size': 1024 * 64,\n",
        "    'aug': 'pyramid'\n",
        "})\n",
        "\n",
        "run(args)"
      ],
      "metadata": {
        "id": "7fzF5aBIejaR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "72dca445-05eb-4a35-e6d0-e24446728e0d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: 51752.0\n",
            "Epoch: 1,  train: 0.1790611,  valid: 0.0762777,  test: 0.0586178,  Correct and smooth...  train: 0.9546739,  valid: 0.6909292,  test: 0.6665226,  loss: 3.5972607, \n",
            "Epoch: 2,  train: 0.2774986,  valid: 0.3003121,  test: 0.2692221,  Correct and smooth...  train: 0.9540031,  valid: 0.7058291,  test: 0.6942987,  loss: 3.2314862, \n",
            "Epoch: 3,  train: 0.2997988,  valid: 0.2918554,  test: 0.2626381,  Correct and smooth...  train: 0.9524527,  valid: 0.6939159,  test: 0.6725099,  loss: 3.0859137, \n",
            "Epoch: 4,  train: 0.2750135,  valid: 0.2647740,  test: 0.2364463,  Correct and smooth...  train: 0.9527056,  valid: 0.6978087,  test: 0.6774479,  loss: 2.9574539, \n",
            "Epoch: 5,  train: 0.2913757,  valid: 0.3032652,  test: 0.2742423,  Correct and smooth...  train: 0.9537172,  valid: 0.7071043,  test: 0.6925498,  loss: 2.8225725, \n",
            "Epoch: 6,  train: 0.3204605,  valid: 0.3265546,  test: 0.3017098,  Correct and smooth...  train: 0.9538492,  valid: 0.7089500,  test: 0.6963768,  loss: 2.6955177, \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4bc0a01ada00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m })\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-22d75ad66093>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_smooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-44bc96668ad1>\u001b[0m in \u001b[0;36mtrain_node\u001b[0;34m(model, data, split_node, optimizer, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deep residual SGC\n",
        "args = objectview({\n",
        "    'hidden_channels': 32,\n",
        "    'dropout': 0.5,\n",
        "    'K': 7,\n",
        "    'lr': 5e-3,\n",
        "    'epochs': 1000,\n",
        "    'batch_size': 1024 * 64,\n",
        "    'aug': 'deepres'\n",
        "})\n",
        "\n",
        "run(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W1Ot3B__eXjz",
        "outputId": "81bb428c-4362-4f7c-d7b0-495462dcdf53"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: 5589.0\n",
            "Epoch: 1,  train: 0.1099394,  valid: 0.2297393,  test: 0.2155628,  Correct and smooth...  train: 0.9589844,  valid: 0.6768348,  test: 0.6712137,  loss: 17579930173101.8378906, \n",
            "Epoch: 2,  train: 0.1099504,  valid: 0.2297393,  test: 0.2155628,  Correct and smooth...  train: 0.9589624,  valid: 0.6768012,  test: 0.6710697,  loss: 9327950344702.2597656, \n",
            "Epoch: 3,  train: 0.0769180,  valid: 0.1495017,  test: 0.2208711,  Correct and smooth...  train: 0.9572470,  valid: 0.6562972,  test: 0.6113409,  loss: 6559585797450.8085938, \n",
            "Epoch: 4,  train: 0.1099394,  valid: 0.2297393,  test: 0.2155628,  Correct and smooth...  train: 0.9589844,  valid: 0.6768348,  test: 0.6712137,  loss: 5737388383487.3437500, \n",
            "Epoch: 5,  train: 0.1099394,  valid: 0.2297393,  test: 0.2155628,  Correct and smooth...  train: 0.9589844,  valid: 0.6768348,  test: 0.6712137,  loss: 4378812693451.1791992, \n",
            "Epoch: 6,  train: 0.1099724,  valid: 0.2297393,  test: 0.2155628,  Correct and smooth...  train: 0.9589844,  valid: 0.6768684,  test: 0.6711520,  loss: 3929605898408.9907227, \n",
            "Epoch: 7,  train: 0.1099614,  valid: 0.2297393,  test: 0.2155628,  Correct and smooth...  train: 0.9589734,  valid: 0.6767341,  test: 0.6710491,  loss: 3223519527942.1591797, \n",
            "Epoch: 8,  train: 0.1099614,  valid: 0.2297393,  test: 0.2155628,  Correct and smooth...  train: 0.9589844,  valid: 0.6768012,  test: 0.6710491,  loss: 2941446614481.0063477, \n",
            "Epoch: 9,  train: 0.1099614,  valid: 0.2297393,  test: 0.2155628,  Correct and smooth...  train: 0.9589844,  valid: 0.6768012,  test: 0.6712137,  loss: 2283255686981.1508789, \n",
            "Epoch: 10,  train: 0.1099394,  valid: 0.2297393,  test: 0.2155628,  Correct and smooth...  train: 0.9589844,  valid: 0.6768012,  test: 0.6711931,  loss: 1942027499091.8703613, \n",
            "Epoch: 11,  train: 0.1101263,  valid: 0.2300413,  test: 0.2158097,  Correct and smooth...  train: 0.9590064,  valid: 0.6761301,  test: 0.6699792,  loss: 1547168161643.4575195, \n",
            "Epoch: 12,  train: 0.1099614,  valid: 0.2296386,  test: 0.2156040,  Correct and smooth...  train: 0.9591054,  valid: 0.6738146,  test: 0.6679423,  loss: 1299497964087.8217773, \n",
            "Epoch: 13,  train: 0.1099504,  valid: 0.2297057,  test: 0.2156040,  Correct and smooth...  train: 0.9590834,  valid: 0.6737810,  test: 0.6684361,  loss: 1016749524024.6042480, \n",
            "Epoch: 14,  train: 0.0787324,  valid: 0.1496359,  test: 0.2208917,  Correct and smooth...  train: 0.9569391,  valid: 0.6506930,  test: 0.6041191,  loss: 736952672613.5290527, \n",
            "Epoch: 15,  train: 0.1116218,  valid: 0.2300077,  test: 0.2158920,  Correct and smooth...  train: 0.9590834,  valid: 0.6706265,  test: 0.6653499,  loss: 645481407024.5701904, \n",
            "Epoch: 16,  train: 0.1114679,  valid: 0.2304775,  test: 0.2162212,  Correct and smooth...  train: 0.9589954,  valid: 0.6704923,  test: 0.6655350,  loss: 433589135732.9102783, \n",
            "Epoch: 17,  train: 0.1111820,  valid: 0.2302091,  test: 0.2159537,  Correct and smooth...  train: 0.9590064,  valid: 0.6745193,  test: 0.6688476,  loss: 509857180074.2378540, \n",
            "Epoch: 18,  train: 0.1115009,  valid: 0.2302762,  test: 0.2160566,  Correct and smooth...  train: 0.9589734,  valid: 0.6735461,  test: 0.6680658,  loss: 467475294697.1139526, \n",
            "Epoch: 19,  train: 0.1439945,  valid: 0.2917212,  test: 0.2800239,  Correct and smooth...  train: 0.9588854,  valid: 0.6474714,  test: 0.6288295,  loss: 401253745645.1168823, \n",
            "Epoch: 20,  train: 0.1111160,  valid: 0.2301420,  test: 0.2158097,  Correct and smooth...  train: 0.9589074,  valid: 0.6744186,  test: 0.6683332,  loss: 367495899425.1354980, \n",
            "Epoch: 21,  train: 0.1105112,  valid: 0.2301420,  test: 0.2157068,  Correct and smooth...  train: 0.9589294,  valid: 0.6719353,  test: 0.6667695,  loss: 348766695917.1450806, \n",
            "Epoch: 22,  train: 0.1444013,  valid: 0.2491359,  test: 0.2298829,  Correct and smooth...  train: 0.9581817,  valid: 0.6453908,  test: 0.6078843,  loss: 274105078724.7158508, \n",
            "Epoch: 23,  train: 0.1137551,  valid: 0.2306118,  test: 0.2165710,  Correct and smooth...  train: 0.9591164,  valid: 0.6709621,  test: 0.6660494,  loss: 265937811684.3762817, \n",
            "Epoch: 24,  train: 0.0821632,  valid: 0.1513809,  test: 0.2220233,  Correct and smooth...  train: 0.9571041,  valid: 0.6513977,  test: 0.6071436,  loss: 254038665738.6238403, \n",
            "Epoch: 25,  train: 0.1171309,  valid: 0.2341354,  test: 0.2178466,  Correct and smooth...  train: 0.9592813,  valid: 0.6669687,  test: 0.6623665,  loss: 280194063036.6677246, \n",
            "Epoch: 26,  train: 0.1420921,  valid: 0.2471224,  test: 0.2290394,  Correct and smooth...  train: 0.9584016,  valid: 0.6498205,  test: 0.6160525,  loss: 294250026076.4111938, \n",
            "Epoch: 27,  train: 0.1110390,  valid: 0.2302762,  test: 0.2161801,  Correct and smooth...  train: 0.9590174,  valid: 0.6710628,  test: 0.6652264,  loss: 240838469300.6618347, \n",
            "Epoch: 28,  train: 0.1211995,  valid: 0.2397060,  test: 0.2207271,  Correct and smooth...  train: 0.9592153,  valid: 0.6610960,  test: 0.6566467,  loss: 200237932011.5348511, \n",
            "Epoch: 29,  train: 0.1474033,  valid: 0.3077284,  test: 0.2998169,  Correct and smooth...  train: 0.9583466,  valid: 0.6464311,  test: 0.6190976,  loss: 155740754814.9936218, \n",
            "Epoch: 30,  train: 0.1130513,  valid: 0.2317192,  test: 0.2167767,  Correct and smooth...  train: 0.9589074,  valid: 0.6722038,  test: 0.6670782,  loss: 167918304289.8364563, \n",
            "Epoch: 31,  train: 0.1260158,  valid: 0.2382295,  test: 0.2213032,  Correct and smooth...  train: 0.9581707,  valid: 0.6638813,  test: 0.6502891,  loss: 283750507397.7946777, \n",
            "Epoch: 32,  train: 0.1362972,  valid: 0.2724924,  test: 0.2593050,  Correct and smooth...  train: 0.9592373,  valid: 0.6400550,  test: 0.6367714,  loss: 240933834567.2901917, \n",
            "Epoch: 33,  train: 0.1450831,  valid: 0.2954462,  test: 0.2842006,  Correct and smooth...  train: 0.9589844,  valid: 0.6476727,  test: 0.6288706,  loss: 171341518284.6598206, \n",
            "Epoch: 34,  train: 0.1106981,  valid: 0.2299742,  test: 0.2157686,  Correct and smooth...  train: 0.9590284,  valid: 0.6747542,  test: 0.6684567,  loss: 250392399504.7873230, \n",
            "Epoch: 35,  train: 0.1242014,  valid: 0.2491359,  test: 0.2442236,  Correct and smooth...  train: 0.9594132,  valid: 0.6655928,  test: 0.6380470,  loss: 255331911793.3886719, \n",
            "Epoch: 36,  train: 0.0503733,  valid: 0.0967818,  test: 0.0962904,  Correct and smooth...  train: 0.9574339,  valid: 0.6669351,  test: 0.6416271,  loss: 194463339012.8361816, \n",
            "Epoch: 37,  train: 0.1118417,  valid: 0.2307124,  test: 0.2160978,  Correct and smooth...  train: 0.9589294,  valid: 0.6742173,  test: 0.6685596,  loss: 294276999204.0096436, \n",
            "Epoch: 38,  train: 0.0785784,  valid: 0.1501728,  test: 0.2212003,  Correct and smooth...  train: 0.9570271,  valid: 0.6547535,  test: 0.6086456,  loss: 337298387977.9876709, \n",
            "Epoch: 39,  train: 0.1105332,  valid: 0.2298735,  test: 0.2156451,  Correct and smooth...  train: 0.9589514,  valid: 0.6761972,  test: 0.6703907,  loss: 465709130431.4376831, \n",
            "Epoch: 40,  train: 0.1113029,  valid: 0.2301755,  test: 0.2158303,  Correct and smooth...  train: 0.9591164,  valid: 0.6728749,  test: 0.6677366,  loss: 538231410816.7698975, \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-159f18963778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m })\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-22d75ad66093>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_smooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-44bc96668ad1>\u001b[0m in \u001b[0;36mtrain_node\u001b[0;34m(model, data, split_node, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# nll_loss for multi-class classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGC with jumping knowledge\n",
        "args = objectview({\n",
        "    'hidden_channels': 64,\n",
        "    'dropout': 0.5,\n",
        "    'K': 5,\n",
        "    'lr': 5e-3,\n",
        "    'epochs': 1000,\n",
        "    'batch_size': 1024 * 64,\n",
        "    'aug': 'jk'\n",
        "})\n",
        "\n",
        "run(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        },
        "id": "_ajO_sG7qEP8",
        "outputId": "69e77bfa-6d93-46e5-dc5a-cfb2eeddd31c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: 10856.0\n",
            "Epoch: 1,  train: 0.0770610,  valid: 0.1496023,  test: 0.2209740,  Correct and smooth...  train: 0.9534643,  valid: 0.6809960,  test: 0.6535399,  loss: 3.6878890, \n",
            "Epoch: 2,  train: 0.1836245,  valid: 0.1917178,  test: 0.2499434,  Correct and smooth...  train: 0.9529145,  valid: 0.6778080,  test: 0.6462358,  loss: 3.4743524, \n",
            "Epoch: 3,  train: 0.1912119,  valid: 0.1082922,  test: 0.1080386,  Correct and smooth...  train: 0.9526506,  valid: 0.6780764,  test: 0.6459684,  loss: 3.2807693, \n",
            "Epoch: 4,  train: 0.1794350,  valid: 0.0772174,  test: 0.0592350,  Correct and smooth...  train: 0.9533434,  valid: 0.6887815,  test: 0.6621196,  loss: 3.1471150, \n",
            "Epoch: 5,  train: 0.1881110,  valid: 0.0999027,  test: 0.0796041,  Correct and smooth...  train: 0.9538492,  valid: 0.6985805,  test: 0.6776125,  loss: 3.1022303, \n",
            "Epoch: 6,  train: 0.2434436,  valid: 0.2263163,  test: 0.2011193,  Correct and smooth...  train: 0.9537832,  valid: 0.7033122,  test: 0.6849577,  loss: 3.0825809, \n",
            "Epoch: 7,  train: 0.2684928,  valid: 0.2784657,  test: 0.2503961,  Correct and smooth...  train: 0.9534094,  valid: 0.7029431,  test: 0.6846697,  loss: 3.0354487, \n",
            "Epoch: 8,  train: 0.2723194,  valid: 0.2857478,  test: 0.2570417,  Correct and smooth...  train: 0.9528156,  valid: 0.6985805,  test: 0.6779005,  loss: 2.9810956, \n",
            "Epoch: 9,  train: 0.2757172,  valid: 0.2917212,  test: 0.2717116,  Correct and smooth...  train: 0.9524197,  valid: 0.6910970,  test: 0.6669341,  loss: 2.9429256, \n",
            "Epoch: 10,  train: 0.2812593,  valid: 0.3057485,  test: 0.3021007,  Correct and smooth...  train: 0.9522987,  valid: 0.6881103,  test: 0.6608234,  loss: 2.9044525, \n",
            "Epoch: 11,  train: 0.2768058,  valid: 0.2948086,  test: 0.2857025,  Correct and smooth...  train: 0.9524197,  valid: 0.6896205,  test: 0.6629220,  loss: 2.8598560, \n",
            "Epoch: 12,  train: 0.2743427,  valid: 0.2869224,  test: 0.2679053,  Correct and smooth...  train: 0.9527606,  valid: 0.6946542,  test: 0.6715018,  loss: 2.8059165, \n",
            "Epoch: 13,  train: 0.2798958,  valid: 0.2943387,  test: 0.2708886,  Correct and smooth...  train: 0.9529805,  valid: 0.7021377,  test: 0.6804724,  loss: 2.7539713, \n",
            "Epoch: 14,  train: 0.2884508,  valid: 0.3044733,  test: 0.2773286,  Correct and smooth...  train: 0.9529915,  valid: 0.7062653,  test: 0.6888258,  loss: 2.6990185, \n",
            "Epoch: 15,  train: 0.2994249,  valid: 0.3145408,  test: 0.2862786,  Correct and smooth...  train: 0.9530795,  valid: 0.7076076,  test: 0.6932082,  loss: 2.6458583, \n",
            "Epoch: 16,  train: 0.3144346,  valid: 0.3280983,  test: 0.3008456,  Correct and smooth...  train: 0.9530135,  valid: 0.7079768,  test: 0.6940724,  loss: 2.5936057, \n",
            "Epoch: 17,  train: 0.3330621,  valid: 0.3467902,  test: 0.3235808,  Correct and smooth...  train: 0.9529915,  valid: 0.7081446,  test: 0.6928585,  loss: 2.5412275, \n",
            "Epoch: 18,  train: 0.3530311,  valid: 0.3710192,  test: 0.3492583,  Correct and smooth...  train: 0.9528486,  valid: 0.7078761,  test: 0.6908833,  loss: 2.4902040, \n",
            "Epoch: 19,  train: 0.3719335,  valid: 0.3936374,  test: 0.3728576,  Correct and smooth...  train: 0.9528815,  valid: 0.7080103,  test: 0.6902249,  loss: 2.4376921, \n",
            "Epoch: 20,  train: 0.3863054,  valid: 0.4081345,  test: 0.3909841,  Correct and smooth...  train: 0.9529475,  valid: 0.7089835,  test: 0.6910685,  loss: 2.3910050, \n",
            "Epoch: 21,  train: 0.3968067,  valid: 0.4199470,  test: 0.4016213,  Correct and smooth...  train: 0.9529805,  valid: 0.7099903,  test: 0.6922412,  loss: 2.3405493, \n",
            "Epoch: 22,  train: 0.4064393,  valid: 0.4290748,  test: 0.4114767,  Correct and smooth...  train: 0.9532774,  valid: 0.7116011,  test: 0.6934963,  loss: 2.2962877, \n",
            "Epoch: 23,  train: 0.4162589,  valid: 0.4362563,  test: 0.4192540,  Correct and smooth...  train: 0.9531674,  valid: 0.7121715,  test: 0.6961504,  loss: 2.2536493, \n",
            "Epoch: 24,  train: 0.4249348,  valid: 0.4454512,  test: 0.4274222,  Correct and smooth...  train: 0.9530575,  valid: 0.7127085,  test: 0.6988663,  loss: 2.2137074, \n",
            "Epoch: 25,  train: 0.4334898,  valid: 0.4528340,  test: 0.4361253,  Correct and smooth...  train: 0.9530025,  valid: 0.7142522,  test: 0.7000391,  loss: 2.1778417, \n",
            "Epoch: 26,  train: 0.4416490,  valid: 0.4601832,  test: 0.4448697,  Correct and smooth...  train: 0.9531344,  valid: 0.7144199,  test: 0.7002448,  loss: 2.1411920, \n",
            "Epoch: 27,  train: 0.4483896,  valid: 0.4679352,  test: 0.4537580,  Correct and smooth...  train: 0.9532444,  valid: 0.7150240,  test: 0.6999362,  loss: 2.1090352, \n",
            "Epoch: 28,  train: 0.4552512,  valid: 0.4732374,  test: 0.4603625,  Correct and smooth...  train: 0.9533214,  valid: 0.7155945,  test: 0.7008415,  loss: 2.0768275, \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-38b8ef7b4d96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m })\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-22d75ad66093>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_smooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-36ceab7c18a0>\u001b[0m in \u001b[0;36mtest_node\u001b[0;34m(model, data, split_node, evaluator, correct_smooth)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_soft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mresult_smoothed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_smoothed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-36ceab7c18a0>\u001b[0m in \u001b[0;36mget_metrics\u001b[0;34m(y_pred, y_true, split_node, evaluator)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     train_acc = evaluator.eval({\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;34m'y_true'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     })[f'acc']\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}